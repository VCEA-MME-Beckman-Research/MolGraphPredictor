# -*- coding: utf-8 -*-
"""Untitled15.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_BHSD0Ugh9ZRtmSdeqzo-qEAZABibKpQ
"""

!pip install tensorflow
!pip install rdkit
!pip install networkx

# Commented out IPython magic to ensure Python compatibility.
import fileinput
from google.colab import drive
drive.mount('/content/drive/')

# # Move to the shared working directory and list what is there
# %cd /content/drive/Shareddrives/Computational-Materials-Science-Group/Graph-Neural-Networks/Linker-data

import pickle
import rdkit, rdkit.Chem, rdkit.Chem.rdDepictor, rdkit.Chem.Draw
import numpy as np
import tensorflow as tf
import matplotlib.pyplot as plt
from sklearn.preprocessing import normalize

with open('GCNN-data-2.pickle', 'rb') as f:
  raw_data = pickle.load(f)

print(raw_data.keys())

n_features = 9
def gen_smiles2graph(sml):
  m = rdkit.Chem.MolFromSmiles(sml)
  m = rdkit.Chem.AddHs(m)
  order_string = {
      rdkit.Chem.rdchem.BondType.SINGLE: 1,
      rdkit.Chem.rdchem.BondType.DOUBLE: 2,
      rdkit.Chem.rdchem.BondType.TRIPLE: 3,
      rdkit.Chem.rdchem.BondType.AROMATIC: 4,
  }

  N = len(list(m.GetAtoms()))
  nodes = (np.zeros((N, n_features)))
  for i in m.GetAtoms():
    #[index of atom in molecule, atomic number]
    nodes[i.GetIdx(), i.GetAtomicNum()] = 1

  adj = np.zeros((N, N))
  for j in m.GetBonds():
    u = min(j.GetBeginAtomIdx(), j.GetEndAtomIdx())
    v = max(j.GetBeginAtomIdx(), j.GetEndAtomIdx())
    order = j.GetBondType()
    if order in order_string:
      order = order_string[order]
    else:
      raise Warning("Ignoring bond order" + order)
    adj[u, v] = 1
    adj[v, u] = 1
  adj += np.eye(N)
  return nodes, adj

class GCNLayer(tf.keras.layers.Layer):
    """Implementation of GCN as layer"""

    def __init__(self, activation=None, **kwargs):
        # constructor, which just calls super constructor
        # and turns requested activation into a callable function
        super(GCNLayer, self).__init__(**kwargs)
        self.activation = tf.keras.activations.get(activation)

    def build(self, input_shape):
        # create trainable weights
        node_shape, adj_shape = input_shape
        self.w = self.add_weight(shape=(node_shape[2], node_shape[2]), name="w")

    def call(self, inputs):
        # split input into nodes, adj
        nodes, adj = inputs
        # compute degree
        degree = tf.reduce_sum(adj, axis=-1)
        # GCN equation
        new_nodes = tf.einsum("bi,bij,bjk,kl->bil", 1 / degree, adj, nodes, self.w)
        out = self.activation(new_nodes)
        return out, adj

class GRLayer(tf.keras.layers.Layer):
    """A GNN layer that computes average over all node features"""

    def __init__(self, name="GRLayer", **kwargs):
        super(GRLayer, self).__init__(name=name, **kwargs)

    def call(self, inputs):
        nodes, adj = inputs
        reduction = tf.reduce_sum(nodes, axis=1)
        return reduction

ninput = tf.keras.Input(
    (
        None,
        n_features,
    )
)
ainput = tf.keras.Input(
    (
        None,
        None,
    )
)
# GCN block
x = GCNLayer("relu")([ninput, ainput])
x = GCNLayer("relu")(x)
x = GCNLayer("relu")(x)
x = GCNLayer("relu")(x)
# reduce to graph features
x = GRLayer()(x)
# standard layers (the readout)
x = tf.keras.layers.Dense(16, "tanh")(x)
x = tf.keras.layers.Dense(1)(x)
model = tf.keras.Model(inputs=(ninput, ainput), outputs=x)

#print(raw_data['moment'])

first_moment_value = []
second_moment_value = []
third_moment_value = []
raw_data['first_moment_normalized'] = []
for key, value in raw_data['moment'].items():
  first_moment_value.append(raw_data['moment'][key][0])
  second_moment_value.append(raw_data['moment'][key][1])
  third_moment_value.append(raw_data['moment'][key][2])

first_moment_list = np.array(first_moment_value)
first_moment_list -= np.min(first_moment_list)
first_moment_list /= np.max(first_moment_list)
# second_moment_list = np.array(second_moment_value).reshape(-1, 1)
# third_moment_list = np.array(third_moment_value).reshape(-1, 1)
# first_moment_normalized = list(normalize(first_moment_list))
# second_moment_normalized = list(normalize(second_moment_list))
# third_moment_normalized = list(normalize(third_moment_list))

# print(first_moment_normalized[0][1])
print(first_moment_list)

print(second_moment_normalized[0][1])

def example():
  k = 0
  for i in raw_data['smiles'].values():
    graph = gen_smiles2graph(i)
    mom = first_moment_list[k]
    k += 1
    yield graph, mom


data = tf.data.Dataset.from_generator(
    example,
    output_types=((tf.float32, tf.float32), tf.float32),
    output_shapes=(
        (tf.TensorShape([None, n_features]), tf.TensorShape([None, None])),
        tf.TensorShape([]),
    ),
)

# for i in raw_data['moment'].values():
#   print(i)

test_data = data.take(1928)
val_data = data.skip(1928).take(1928)
train_data = data.skip(3856)

model.compile("adam", loss="mean_squared_error")
result = model.fit(train_data.batch(1), validation_data=val_data.batch(1), epochs=10)

plt.plot(result.history["loss"], label="training")
plt.plot(result.history["val_loss"], label="validation")
plt.legend()
plt.xlabel("Epoch")
plt.ylabel("Loss")
plt.show()

yhat = model.predict(test_data.batch(1), verbose=0)[:, 0]
test_y = [y for x, y in test_data]
plt.figure()
plt.plot(test_y, test_y, "-")
plt.plot(test_y, yhat, ".")
plt.text(
    min(test_y) + 1,
    max(test_y) - 2,
    f"correlation = {np.corrcoef(test_y, yhat)[0,1]:.3f}",
)
plt.text(
    min(test_y) + 1,
    max(test_y) - 3,
    f"loss = {np.sqrt(np.mean((test_y - yhat)**2)):.3f}",
)
plt.title("Testing Data")
plt.show()